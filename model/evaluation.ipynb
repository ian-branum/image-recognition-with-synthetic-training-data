{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had to manually run these from within the Sagemaker notebook venv\n",
    "#!apt-get update\n",
    "#!python3 -m pip install --upgrade pip\n",
    "#!pip install opencv-python\n",
    "#!apt-get install -y libglib2.0-0\n",
    "#!apt-get install -y libsm6 libxext6 libxrender-dev\n",
    "#!pip install matplotlib --upgrade\n",
    "#!apt-get install -y python3-skimage\n",
    "#!pip install scikit-image==0.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "print(ROOT_DIR)\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "from src.VehiclesDataset import VehiclesDataset\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, 'models/mask_rcnn_coco.h5')\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehiclesConfig(Config):\n",
    "    NAME = \"vehicles\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "    NUM_CLASSES = 1 + 2  # background + 2 shapes\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    #BACKBONE = \"resnet50\" -- default is resnet101\n",
    "\n",
    "    #These are for BW\n",
    "    #IMAGE_CHANNEL_COUNT = 1\n",
    "    #MEAN_PIXEL = 1\n",
    "    \n",
    "    #Faster epochs?\n",
    "    #STEPS_PER_EPOCH = 100\n",
    "config = VehiclesConfig()\n",
    "#config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dir = '../../data/color/train'\n",
    "val_dataset_dir = '../../data/color/val'\n",
    "mask_dir = '../../data/masks/'\n",
    "\n",
    "\n",
    "# Training dataset\n",
    "dataset_train = VehiclesDataset()\n",
    "dataset_train.load_images(train_dataset_dir, mask_dir)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = VehiclesDataset()\n",
    "dataset_val.load_images(val_dataset_dir, mask_dir)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /root/image-recognition-with-synthetic-training-data/model/models/mask_rcnn_vehicles-clean-start-5.h5\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(VehiclesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_vehicles-clean-start-5.h5\")\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "mAP:  0.887\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 to determine mean precision\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 1000)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_test(directory='../../data/test/'):\n",
    "    '''\n",
    "    Run predict against all images in a directory and return a dataframe with predicted and actual classes\n",
    "    '''\n",
    "    preds = []\n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        #print(filename)\n",
    "        if(os.path.isdir(directory + '/' + filename)):\n",
    "            continue\n",
    "        img = Image.open(directory + '/' + filename)\n",
    "        img = img.resize((256, 256))\n",
    "        img_as_arr = np.array(img)\n",
    "        results = model.detect([img_as_arr], verbose=0)\n",
    "        if('car' in filename):\n",
    "            actual_class = 1\n",
    "        elif('pickup' in filename):\n",
    "            actual_class = 2\n",
    "        else:\n",
    "            actual_class = 0\n",
    "        if(len(results[0]['class_ids']) > 0):\n",
    "            class_id = results[0]['class_ids'][0]\n",
    "            score = results[0]['scores'][0]\n",
    "            preds.append([filename, actual_class, class_id, float(score)])\n",
    "        else:\n",
    "            preds.append([filename, actual_class, 0, 0])\n",
    "    return preds\n",
    "preds_new2 = score_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  8,  1],\n",
       "       [ 5, 32, 12],\n",
       "       [ 6, 35,  9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(preds_new2)\n",
    "confusion_matrix(df[1], df[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.73        21\n",
      "           1       0.43      0.73      0.54        44\n",
      "           2       0.41      0.20      0.27        44\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       109\n",
      "   macro avg       0.61      0.50      0.51       109\n",
      "weighted avg       0.53      0.49      0.47       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df[1], df[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-1.15-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
